---
layout: post
title: Logstash
categories: BigData
---

## Install JAVA ENV

```
wget -c --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u131-b11/d54c1d3a095b4ff2b6607d096fa80163/jdk-8u131-linux-x64.tar.gz
tar xf jdk-8u131-linux-x64.tar.gz -C /usr/local/
cat >> /etc/profile <<EOF
export JAVA_HOME=/usr/local/jdk1.8.0_131
export CLASSPATH=.:\$JAVA_HOME/lib/dt.jar:\$JAVA_HOME/lib/tools.jar
export PATH=\$JAVA_HOME/bin:\$PATH
EOF
source /etc/profile
```

-------------------------------------------

## Install Logstash

```
wget https://download.elastic.co/logstash/logstash/logstash-2.4.0.tar.gz
tar xf logstash-2.4.0.tar.gz -C /usr/local/
ln -s /usr/local/logstash-2.4.0 /usr/local/logstash
sed -i 's#source.*#source "https://ruby.taobao.org/"#g' /usr/local/logstash/Gemfile
```

-----------------------------------

## Install Plugin

```
cd /usr/local/logstash
./bin/logstash-plugin install logstash-input-elasticsearch
./bin/logstash-plugin install logstash-output-elasticsearch
./bin/logstash-plugin install logstash-input-kafka
./bin/logstash-plugin install logstash-output-kafka
./bin/logstash-plugin install logstash-output-webhdfs
./bin/logstash-plugin list|egrep "kafka|webhdfs|elasticsearch"
```

------------------------

## Configure Logstash

> mkdir /usr/local/logstash/conf/

* File --> Kafka/File

```
#
input {
    file {
        path => ["/$PATH/**/*access.log"]
        sincedb_path => "/$PATH/logstash/sincedb"
        start_position => "beginning"
		codec => "plain"
        discover_interval => 10
        close_older => 3600
        ignore_older => 86400
        sincedb_write_interval => 5
        stat_interval => 1
    }
}

output {
    kafka {
        bootstrap_servers => "test-kafka00:9092,test-kafka01:9092,test-kafka02:9092"
        topic_id => "test_topic"
        codec => plain {
                        format => "%{message}"
                }
    }
    file {
        path => "/home/mtime/logs/accesslog/vip.mtime.com/from_kafka.log"
        message_format => "%{message}"
    }
}
```

> sincedb_path: sincedb存放位置；用于记录文件读取的偏移量

> start_position: 在没有sincedb记录的前提下；默认从文件尾部开始读取（tail -f）；beginning 从头读取

> codec: plain表示文本模式；json表示按照json格式处理

> discover_interval: 没隔多久检测一次被监听目录下是否有新文件生成

> close_older: 默认3600s文件没有变化时；释放文件监听句柄

> ignore_older: 如果一个文件的最后修改时间超过这个值就忽略这个文件；默认是86400s（1day）

> sincedb_write_interval: 每隔多久写一次sincedb记录；关系到丢失数据的容忍度；不推荐太大

> stat_interval: 每隔多久检查一次被监听文件状态是否有更新；默认1s

> message_format: logstash-output-file插件无法使用codec插件；所以用此参数；

> %{message}: 获取原始的日志内容；不添加任何多余字段；

* Kafka --> ES

```
#
input {
    kafka {
        zk_connect => "zookeeper01:2181,zookeeper03:2181,zookeeper03:2181"
        group_id => "logstash-kafka-es"
        topic_id => "test_kafka"
        codec => plain
        reset_beginning => false
        consumer_threads => 4
        decorate_events => false
    }
}

filter {
        grok {
                match => {
                        "message" => "^(?<hostname>.+?)\s(?<modulname>.+?)\s(?<remote_addr>.+?)\s\-\s(?<remote_user>.+?)\s\[(?<localtime>.+?)\]\s\"(?<request>.+?)\"\s(?<status>.+?)\s(?<body_bytes_sent>.+?)\s\"(?<http_referer>.+?)\"\s\"(?<http_user_agent>.+?)\"\s\"(?<http_x_forwarded_for>.+?)\"\s\"(?<upstream_addr>.+?)\"\s\"(?<upstream_response_time>.+?)\"\s\"(?<request_time>.+?)\"\s\"(?<http_cookie>.+?)\""
                }
        }
        mutate {
                convert => [
                "status", "integer",
                "body_bytes_sent" , "integer",
                "upstream_response_time", "float",
                "request_time", "float"
                ]
        }
}

output {
        if [modulname] {
           elasticsearch {
                workers => 2
                hosts => ["elasticsearch01:9210","1elasticsearch02:9210","elasticsearch03:9210"]
                index => "logstash-%{modulname}-%{+YYYY.MM.dd}"
                flush_size => 10000
                idle_flush_time => 60
                template_overwrite => true
            }
        } else {
           file {
                path => "/$PATH/logstash/logstash_kafka_es_error.log"
                message_format => "%{message}"
           }
        }
        stdout{codec => rubydebug}
}
```

> group_id: 消费者分组 可以通过组ID去指定,不同的组之间消费是相互不受影响的,相互隔离

> topic_id: 指定消费某个topic

> reset_beginning: logstash启动后从什么位置开始读取数据；默认是结束位置；如果要导入原有数据；将此值设置为true

> decorate_events: 输出自身一些消息；类似消费消息大小；topic来源等信息；你默认false

> rebalance_max_retries: 注册节点的重试次数；多个logstash消费同一个topic；需要向zk注册partitons归属于哪个logstash消费；

> grok: 用正则表达式将纯文本内容变成json格式

> mutate: 更改json格式中指定key的value的字符类型；integer整数 string字符串 float浮点数


* Kafka --> HDFS

```
#
input {
    kafka {
        zk_connect => "zookeeper01:2181,zookeeper03:2181,zookeeper03:2181"
        group_id => "test_kafka"
        topic_id => "test_kafka_id"
        codec => plain
        reset_beginning => false
        consumer_threads => 4
        decorate_events => false
    }
}

filter {
        grok {
                match => {
                        "message" =>"^(?<hostname>.+?)\s(?<modulname>.+?)\s(?<remote_addr>.+?)\s\-\s(?<remote_user>.+?)\s\[(?<Day>.+?)/(?<Month>.+?)/(?<Year>.+?):(?<Hour>.+?):"
                }
        }
        mutate {
                gsub => ["modulname", "\.", ""]
        }
        if [Month] == "Jan" {
                        mutate {
                                update => ["Month","01"]
                        } 
        } else if [Month] == "Feb" {
                        mutate {
                                update => ["Month","02"]
                        }
        } else if [Month] == "Mar" {
                        mutate {
                                update => ["Month","03"]
                        }
        } else if [Month] == "Api" {
                        mutate {
                                update => ["Month","04"]
                        }
        } else if [Month] == "May" {
                        mutate {
                                update => ["Month","05"]
                        }
        } else if [Month] == "Jun" {
                        mutate {
                                update => ["Month","06"]
                        }
        } else if [Month] == "Jul" {
                        mutate {
                                update => ["Month","07"]
                        }
        } else if [Month] == "Aug" {
                        mutate {
                                update => ["Month","08"]
                        }
        } else if [Month] == "Sep" {
                        mutate {
                                update => ["Month","09"]
                        }
        } else if [Month] == "Oct" {
                        mutate {
                                update => ["Month","10"]
                        }
        } else if [Month] == "Nov" {
                        mutate {
                                update => ["Month","11"]
                        }
        } else if [Month] == "Dec" {
                        mutate {
                                update => ["Month","12"]
                        }
        } 
}

output {
        if [modulname] {
           webhdfs {
                workers => 2
                host => "hdfs-host.host.com"
                port => 50070
                user => "loguser"
                path => "/Service-Data/Logs/%{modulname}/dt=%{Year}%{Month}%{Day}/hour=%{Hour}/%{hostname}_%{Year}%{Month}%{Day}%{Hour}.log"
                flush_size => 5000
                compression => "gzip"
                idle_flush_time => 6
                retry_interval => 3
                retry_times => 3
                codec => line {
                        format => "%{message}"
                }
           }
        } else {
           file {
                path => "/$PATH/logstash/logstash_kafka_hdfs_error.log"
                message_format => "%{message}"
           }
        }
        stdout{codec => rubydebug}
}
```

> update: 更新value值

--------------------------------

## Start Logstash

```
cd /usr/local/logstash
nohup ./bin/logstash -f conf/logstash.conf >>/var/log/logstash.debug.log &
```

--------------------------

`推荐用supervisor守护logstash进程`

* [参考supervisor进程守护](http://wangrubin.com/point/supervisor.html)

```
cat >/etc/supervisor/conf/logstash.ini<<EOF
[program:Logstash]
command = /usr/local/logstash/bin/logstash -f /usr/local/logstash/conf/logstash.conf
autostart = true
autorestart = true
startsecs = 3
startretries = 3
stopwaitsecs = 5
user = root
redirect_stderr = true
stdout_logfile = /var/log/supervisor/logstash.log
stdout_logfile_maxbytes = 500MB
stdout_logfile_backups = 0
EOF
```

> supervisorctl update

---------------------

end




