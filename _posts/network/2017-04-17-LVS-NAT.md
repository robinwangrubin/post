---
layout: post
title: LVS 四层负载均衡
categories: Network
---


* content
{:toc}

## 介绍

Linux虚拟服务器（Linux Virtual Server）

我们使用该软件配置LVS时候，不能直接配置内核中的ipvs，而需要使用ipvs的管理工具ipvsadm进行管理；或者使用keepalived管理。

LVS集群负载均衡器接受服务的所有入站客户端计算机请求，并根据调度算法决定哪个集群节点应该处理回复请求。负载均衡器(简称LB)有时也被称为LVS Director(简称Director)。

## LVS工作模式
 
* NAT（Network Address Translation）

* DR（Direct Routing）

* TUN（IP Tunneling）

* FULLNAT（Full Network Address Translation）

## LVS工作原理

### NAT

1. NAT技术将请求的报文（通过DNAT方式改写）和响应的报文（通过SNAT方式改写），通过调度器地址重写然后在转发给内部的服务器，报文返回时改写成原来的用户请求的地址。

2. 只需要在调度器LB上配置WAN公网IP即可，调度器也要有私有LAN IP和内部RS节点通信。

3. 每台内部RS节点的网关地址，必须要配成调度器LB的私有LAN内物理网卡地址（LDIP），这样才能确保数据报文返回时仍然经过调度器LB。

4. 由于请求与响应的数据报文都经过调度器LB，因此，网站访问量大时调度器LB有较大瓶颈，一般要求最多10-20台节点。

5. NAT模式支持对IP及端口的转换，即用户请求10.0.1.1:80，可以通过调度器转换到RS节点的10.0.1.2:8080（DR和TUN模式不具备的）。

6. 所有NAT内部RS节点只需配置私有LAN IP即可。

7. 由于数据包来回都需要经过调度器，因此，要开启内核转发net.ipv4.ip_forward = 1，当然也包括iptables防火墙的forward功能（DR和TUN模式不需要）。


### DR

1. 通过在调度器LB上修改数据包的目的MAC地址实现转发。注意，源IP地址仍然是CIP，目的IP地址仍然是VIP。

2. 请求的报文经过调度器，而RS响应处理后的报文无需经过调度器LB，因此，并发访问量大时使用效率很高（和NAT模式比）。

3. 因DR模式是通过MAC地址的改写机制实现的转发，因此，所有RS节点和调度器LB只能在一个局域网LAN中（小缺点）。

4. 需要注意RS节点的VIP的绑定（lo:vip/32,lo1:vip/32）和ARP抑制问题。

5. RS节点的默认网关不需要是调度器LB的DIP，而直接是IDC机房分配的上级路由器的IP（这是RS带有外网IP地址的情况），理论讲：只要RS可以出网即可，不是必须要配置外网IP。

6. 由于DR模式的调度器仅进行了目的MAC地址的改写，因此，调度器LB无法改变请求的报文的目的端口（和NAT要区别）。

7. 总的来说DR模式效率很高，但是配置也较麻烦，因此，访问量不是特别大的公司可以用haproxy/nginx取代之。这符合运维的原则：简单、易用、高效。日1000-2000W PV或并发请求1万以下都可以考虑用haproxy/nginx（LVS NAT模式）

8. 直接对外的访问业务，例如：web服务做RS节点，RS最好用公网IP地址。如果不直接对外的业务，例如：MySQL,存储系统RS节点，最好只用内部IP地址。


### TUN

1. 负载均衡器通过把请求的报文通过IP隧道（ipip隧道，高级班讲这个）的方式（请求的报文不经过原目的地址的改写(包括MAC)，而是直接封装成另外的IP报文）转发至真实服务器，而真实服务器将响应处理后直接返回给客户端用户。

2. 由于真实服务器将响应处理后的报文直接返回给客户端用户，因此，最好RS有一个外网IP地址，这样效率才会更高。理论上：只要能出网即可，无需外网IP地址。

3. 由于调度器LB只处理入站请求的报文。因此，此集群系统的吞吐量可以提高10倍以上，但隧道模式也会带来一定的系统开销。TUN模式适合LAN/WAN。

4. TUN模式的LAN环境转发不如DR模式效率高，而且还要考虑系统对IP隧道的支持问题。

5. 所有的RS服务器都要绑定VIP，抑制ARP，配置复杂。

6. LAN环境一般多采用DR模式，WAN环境可以用TUN模式，但是当前在WAN环境下，请求转发更多的被haproxy/nginx/DNS调度等代理取代。因此，TUN模式在国内公司实际应用的已经很少。跨机房应用要么拉光纤成局域网，要么DNS调度，底层数据还得同步。

7. 直接对外的访问业务，例如：web服务做RS节点，最好用公网IP地址。不直接对外的业务，例如：MySQL,存储系统RS节点，最好用内部IP地址。

### FULLNAT

> 数据包经过LVS时；同时更改DIP+PORT 和 SIP+PORT 



## 需求

* 使用lvs实现 client访问公网IPA+prot 映射到内网的real server的IPB+port上
* Real server 可以看到client的公网IP地址

`LVS本是用来做负载均衡，但在单real server节点的时候，相当于实现了NAT功能`

## 前提

> lvs有一个内网IP 和 后端real server在同一网段

> 后端的real server 需要将网关指向lvs的内网IP

> lvs上配置了公网VIP用户可以访问到。（即路由可达）



## 模拟测试

`安装LVS`

{% highlight shell %}
modprobe -r ip_vs		#移除lvs模块
echo "options ip_vs conn_tab_bits=20" > /etc/modprobe.d/ip_vs.conf      #优化lvs参数
modprobe ip_vs			#导入lvs模块
yum install ipvsadm -y	#安装IPVS的管理模块，真正实现负载的是工作在内核空间的IPVS
lsmod |grep ip_vs	#有输出则代表内核支持lvs，基本都支持lvs
ulimit -n 65535
sed -i 's#net.ipv4.ip_forward = 0#net.ipv4.ip_forward = 1#g' /etc/sysctl.conf		#开启内核转发
sysctl -p	#使配置生效
{% endhighlight %}


`配置转发策略`

{% highlight shell %}
ipvsadm -C		#清空所有规则
ipvsadm -A -t 128.24.170.256:7000 -s rr		#添加VIP+PORT 负载策略：轮询
ipvsadm -a -t 128.24.170.256:7000  -r 192.168.1.12:7000 -m	#VIP+PORT ----> RIP+PORT 负载模式：NAT
{% endhighlight %}

`保存配置`

{% highlight shell %}
service ipvsadm save
{% endhighlight %}

`开机启动`

{% highlight shell %}
chkconfig ipvsadm on
{% endhighlight %}

`重启`

{% highlight shell %}
service ipvsadm restart
/etc/init.d/ipvsadm restart
{% endhighlight %}

-----------------------------

end


